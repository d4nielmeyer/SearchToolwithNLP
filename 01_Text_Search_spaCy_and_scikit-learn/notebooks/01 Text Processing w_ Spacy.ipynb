{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"searchenv","language":"python","name":"searchenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.11"},"colab":{"name":"01 Text Processing w/ Spacy.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"a2jji3LQ0LAp"},"source":["# **Milestone 1:**\n","Text Processing using Spacy\n"]},{"cell_type":"markdown","metadata":{"id":"OCZ64HgIoKrE"},"source":["### **Setting up the environment**"]},{"cell_type":"code","metadata":{"id":"vC5NiKQsnE0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638811768262,"user_tz":-60,"elapsed":1562,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"98086efb-d8e3-422b-91e8-b00163fb82af"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"nDLDjeBwrxDk"},"source":["###**Importing the required modules**"]},{"cell_type":"code","metadata":{"id":"KF2VkBMxzqmr","executionInfo":{"status":"ok","timestamp":1638811768263,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["# import libraries\n","import json\n","import spacy"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou6WJYn28B_B"},"source":["### **Getting the data**"]},{"cell_type":"code","metadata":{"id":"dVzHTT4Y8YBx","executionInfo":{"status":"ok","timestamp":1638811768263,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["DATA_DIR = '/content/drive/MyDrive/SearchToolwNLP/01_Text_Search_spaCy_and_scikit-learn/data/'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPzUDFeDXNSj","executionInfo":{"status":"ok","timestamp":1638811769013,"user_tz":-60,"elapsed":755,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["# load a spacy language model\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Jlya41XkXNSj","executionInfo":{"status":"ok","timestamp":1638811769014,"user_tz":-60,"elapsed":15,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["# load the json file\n","with open(DATA_DIR + 'data.json', 'r') as outfile:\n","    summaries = json.load(outfile)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IF-hGN80XNSk"},"source":["### **Inspecting the dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOFVl5wPtadG","executionInfo":{"status":"ok","timestamp":1638811769014,"user_tz":-60,"elapsed":15,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"4498030e-7b73-4f9b-dbcb-c90ce6e42289"},"source":["# len of the list\n","print(f'The dataset comprises a list of {len(summaries)} dicts')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset comprises a list of 26 dicts\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSaJ7eiuvFC5","executionInfo":{"status":"ok","timestamp":1638811769014,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"e536dcd8-5e8f-49e3-8534-c9de510fdb67"},"source":["# get the keys\n","print(f'Each entry contains the following {summaries[0].keys()}')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Each entry contains the following dict_keys(['title', 'text', 'url'])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlrZ6cSidPlm","executionInfo":{"status":"ok","timestamp":1638811769015,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"ba0526c6-141e-4b3c-81e7-8d55a050d162"},"source":["# print the first entry\n","print(summaries[0]['title'])\n","print('---')\n","print(summaries[0]['text'])\n","print('---')\n","print(summaries[0]['url'])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Pandemic\n","---\n","A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n","Throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.\n","---\n","https://en.wikipedia.org/wiki/Pandemic\n"]}]},{"cell_type":"markdown","metadata":{"id":"owXXULdKqSaz"},"source":["### **Cleaning the dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLivFG1vXNSl","executionInfo":{"status":"ok","timestamp":1638811769015,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"31e130c0-d767-45e2-a687-f2a5c60faacb"},"source":["# get the text content\n","text = summaries[0]['text']\n","\n","# create a doc object\n","doc = nlp(text.lower())\n","\n","# explore the attributes of each token returned spacy\n","print(doc[:20])\n","print('--------------------------------')\n","for token in doc[:5]:\n","    print(token.text) \n","    print(token.pos_) \n","    print(token.dep_)\n","    print('---')"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\"\n","--------------------------------\n","a\n","DET\n","det\n","---\n","pandemic\n","NOUN\n","nsubj\n","---\n","(\n","PUNCT\n","punct\n","---\n","from\n","ADP\n","prep\n","---\n","greek\n","ADJ\n","amod\n","---\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCFjXk9IdpUF","executionInfo":{"status":"ok","timestamp":1638811769015,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"47667fd5-4da6-46c9-855f-f0d820d015a6"},"source":["# identify unclassified tokens\n","unclassified_tokens = [(token.lemma_, token.dep_) for token in doc if token.dep_ is '']\n","unclassified_tokens[:10]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('\\n', '')]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyLRRLZYXNSm","executionInfo":{"status":"ok","timestamp":1638811769016,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"1ed1af9a-f062-4fa6-bd9a-e61a58153a56"},"source":["# remove stop words and punctuation\n","token_without_sw = [word for word in doc if not word.is_stop and not word.is_punct]\n","token_without_sw[:10]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[pandemic,\n"," greek,\n"," πᾶν,\n"," pan,\n"," δῆμος,\n"," demos,\n"," people,\n"," epidemic,\n"," infectious,\n"," disease]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjxqLr0tXNSn","executionInfo":{"status":"ok","timestamp":1638811769016,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}},"outputId":"94753bca-46c1-43a3-e576-db987fe50557"},"source":["# lemmatize (tokenize) the texts\n","token_lemmas = [token.lemma_ for token in token_without_sw if token.dep_]\n","token_lemmas[:10]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['pandemic',\n"," 'greek',\n"," 'πᾶν',\n"," 'pan',\n"," 'δῆμος',\n"," 'demos',\n"," 'people',\n"," 'epidemic',\n"," 'infectious',\n"," 'disease']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"IkOXYy6RXNSo","executionInfo":{"status":"ok","timestamp":1638811769016,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["# build a tokenizer function\n","def tokenizer(document):\n","    \"\"\"\n","    This function accepts a text string and:\n","    1. Lowercases it\n","    2. Removes redundant tokens\n","    3. Performs token lemmatization\n","    \"\"\"\n","    doc = nlp(document.lower())\n","    token_without_sw = [word for word in doc if not word.is_stop and not word.is_punct]\n","    token_lemmas = [token.lemma_ for token in token_without_sw if token.dep_]  \n","\n","    return token_lemmas"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTuUsyFAwSHh"},"source":["### **Saving the dataset**"]},{"cell_type":"code","metadata":{"id":"jwCvC-1dXNSo","executionInfo":{"status":"ok","timestamp":1638811769016,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel Meyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01875097517544988126"}}},"source":["# save the tokenized texts to file:\n","with open(DATA_DIR + 'summaries.json', 'w') as outfile:\n","    json.dump(summaries, outfile)"],"execution_count":14,"outputs":[]}]}